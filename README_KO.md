<p align="left">
    <a href="README_CN.md">ä¸­æ–‡</a>&nbsp ï½œ &nbspEnglish&nbsp ï½œ &nbsp<a href="README_JA.md">æ—¥æœ¬èª</a> ï½œ &nbsp<a href="README_FR.md">FranÃ§ais</a>
</p>
<br><br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/>
<p>
<br>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/organization/qwen">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://arxiv.org/abs/2309.16609">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href="https://modelscope.cn/studios/qwen/Qwen-14B-Chat-Demo/summary">Demo</a>
<br>
<a href="assets/wechat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp ï½œ &nbsp&nbsp DingTalk (é’‰é’‰) &nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp
</p>
<br><br>

|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |
|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|
| 7B  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4">ğŸ¤—</a>  | <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int8">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>  |
| 14B | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int4">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int8">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B">ğŸ¤—</a> |



ê¸°ë³¸ ì–¸ì–´(base language models)ì¸ **Qwen**, **Qwen-7B**, **Qwen-14B**ì™€ ì±„íŒ… ëª¨ë¸(chat models)ì¸ **Qwen-Chat**ì„ í¬í•¨í•œ **Qwen** ì‹œë¦¬ì¦ˆë¥¼ ì˜¤í”ˆì†ŒìŠ¤í™”í•©ë‹ˆë‹¤. ë§í¬ëŠ” ìœ„ í‘œì— ìˆìœ¼ë©°, í•´ë‹¹ ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ ëª¨ë¸ ì¹´ë“œë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, [Qwenì˜ ê¸°ìˆ  ë³´ê³ ì„œ](https://arxiv.org/abs/2309.16609)ë¥¼ ê³µê°œí•©ë‹ˆë‹¤. ë…¼ë¬¸ ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ í™•ì¸í•´ ì£¼ì„¸ìš”.

ìš”ì•½í•˜ìë©´, ë‹¹ì‚¬ëŠ” ê´‘ë²”ìœ„í•œ ë„ë©”ì¸ê³¼ ì–¸ì–´(ì¤‘êµ­ì–´ì™€ ì˜ì–´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ) ë“±ìœ¼ë¡œ êµ¬ì„± ëœ ìµœëŒ€ 3ì¡° í† í°ì˜ ë‹¤êµ­ì–´ ë°ì´í„°ì— ëŒ€í•´ ì•ˆì •ì ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ê°•ë ¥í•œ ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ìœ¼ë©°, ì±„íŒ…Â·ì½˜í…ì¸  ìƒì„±Â·ì •ë³´ ì¶”ì¶œÂ·ìš”ì•½Â·ë²ˆì—­Â·ì½”ë”©Â·ìˆ˜í•™ ë¬¸ì œ í’€ì´ ë“±ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì—ì´ì „íŠ¸ ì—­í• ì„ í•  ìˆ˜ ìˆìœ¼ë©°, ì½”ë“œ ì¸í„°í”„ë¦¬í„° ì—­í•  ë˜í•œ í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒì˜ ì„ í˜¸ë„ì— ë§ì¶°ì§„ SFT ë° RLHF(ì•„ì§ ì¶œì‹œë˜ì§€ ì•ŠìŒ) ì±„íŒ… ëª¨ë¸ ì—­ì‹œ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì´ ë ˆí¬ì§€í† ë¦¬ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ì†Œê°œí•©ë‹ˆë‹¤.

* Quick Startë¥¼ í†µí•œ Qwenì„ ì´ìš©í•œ ê°„ë‹¨í•œ ì¶”ë¡  ë°©ë²•
* GPTQ ë° KV ìºì‹œ ì–‘ìí™”ë¥¼ í¬í•¨í•œ ì–‘ìí™” ëª¨ë¸ì— ëŒ€í•œ ì„¸ë¶€ ì •ë³´
* ì†ë„ì™€ ë©”ëª¨ë¦¬ ë¹„ìš©ì„ í¬í•¨í•œ ì¶”ë¡  ì„±ëŠ¥ì— ëŒ€í•œ í†µê³„
* ì „ì²´ íŒŒë¼ë¯¸í„° íŠœë‹, LoRA ë° Q-LoRAë¥¼ í¬í•¨í•œ ë¯¸ì„¸ íŠœë‹ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼
* ë°°í¬ ê°€ì´ë“œ (vLLM ë° FastChatì˜ ì˜ˆì‹œ í¬í•¨)
* WebUI, CLI ë°ëª¨ ë“±ì„ í¬í•¨í•œ ë°ëª¨ êµ¬ì¶• ë°©ë²•
* DashScope API ì„œë¹„ìŠ¤ ì†Œê°œ ë° ëª¨ë¸ì— ëŒ€í•œ OpenAI ìŠ¤íƒ€ì¼ API êµ¬ì¶• ê°€ì´ë“œ
* ë„êµ¬ ì‚¬ìš©, ì—ì´ì „íŠ¸ ë° ì½”ë“œ ì¸í„°í”„ë¦¬í„°ë¥¼ ìœ„í•œ Qwenì— ëŒ€í•œ ì •ë³´
* ì¥ë¬¸ ì´í•´ë„ì— ëŒ€í•œ í‰ê°€ ê²°ê³¼
* ë¼ì´ì„¼ìŠ¤
* ê·¸ ì™¸ ê´€ë ¨ëœ ì§€ì‹

ë¬¸ì œê°€ ë°œìƒí•œë‹¤ë©´ [FAQ](FAQ.md)ë¥¼ ì°¸ì¡°í•˜ì—¬ ë„ì›€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆê³ , ì¶”ê°€ì ì¸ ì–´ë ¤ì›€ì´ ìˆìœ¼ì‹œë‹¤ë©´ ì–¸ì œë“ ì§€ ì•Œë ¤ì£¼ì‹­ì‹œìš”. (ë” ë§ì€ ì‚¬ëŒê³¼ ë¬¸ì œë¥¼ ê³µìœ í•˜ê¸° ìœ„í•´ì„œ ì˜ì–´ë¡œ ì‘ì„±í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.) ì €í¬ íŒ€ì„ ë•ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´, ì£¼ì €í•˜ì§€ ë§ˆì‹œê³  í’€ ë¦¬í€˜ìŠ¤íŠ¸ë¥¼ ë³´ë‚´ì£¼ì„¸ìš”! ì €í¬ íŒ€ì€ í•­ìƒ ì €í¬ì˜ ëª¨ë¸ì„ í™ë³´í•˜ê³ ì ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì €í¬ì™€ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê±°ë‚˜ ì»¤í”¼ íƒ€ì„ì„ ê°–ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ë””ìŠ¤ì½”ë“œ ë˜ëŠ” ìœ„ì±—ì— ì°¸ì—¬í•´ì£¼ì„¸ìš”! ì–¸ì œë“ ì§€ í™˜ì˜í•©ë‹ˆë‹¤.
<br><br>

## News and Updates

* 2023.10.17 Int8 quantized model **Qwen-7B-Chat-Int8**, **Qwen-14B-Chat-Int8**ì„ ë¦´ë¦¬ì¦ˆí•˜ì˜€ìŠµë‹ˆë‹¤.
* 2023.9.25 ğŸ”¥ [qwen.cpp](https://github.com/QwenLM/qwen.cpp)ì™€ [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)ë¥¼ í¬í•¨í•˜ì—¬, ModelScope ë° Hugging Faceì— **Qwen-14B**ê³¼ **Qwen-14B-Chat**ë¥¼ ë¦´ë¦¬ì¦ˆí•˜ì˜€ìœ¼ë©°, Codesì™€ **Qwen-7B**, **Qwen-7B-Chat**ëª¨ë¸ë“¤ì˜ ì²´í¬ í¬ì¸íŠ¸ê°€ ì—…ë°ì´íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤. **ìµœì‹  ë²„ì „ì„ Pull í•´ì„œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.**
    - ìƒˆë¡œ ì—…ë°ì´íŠ¸ ëœ **Qwen-7B**ëŠ” ì´ì „ ëª¨ë¸ë³´ë‹¤ ë” ë§ì€ ìˆ˜ì˜ í›ˆë ¨ í† í°ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. í›ˆë ¨ì— ì‚¬ìš©ëœ í† í°ì˜ ìˆ˜ê°€ 2.2Tì—ì„œ 2.4Të¡œ ì¦ê°€í–ˆìœ¼ë©°, ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ëŠ” 2048ì—ì„œ 8192ë¡œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. **Qwen-7B**ì˜ ì¤‘êµ­ì–´ ì§€ì‹ê³¼ ì½”ë”© ëŠ¥ë ¥ì´ ë”ìš± í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
* 2023.9.12 ì „ì²´ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •, LoRA ë° Q-LoRAë¥¼ í¬í•¨í•œ Qwen-7B ëª¨ë¸ì—ì„œ ë¯¸ì„¸ ì¡°ì •ì„ ì§€ì›í•©ë‹ˆë‹¤.
* 2023.8.21 ë‚®ì€ ë©”ëª¨ë¦¬ ë¹„ìš©ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì¶”ë¡  í•  ìˆ˜ ìˆëŠ” Qwen-7B-Chatìš© Int4 ì–‘ìí™” ëª¨ë¸ì¸ **Qwen-7B-Chat-Int4**ë¥¼ ì¶œì‹œí•©ë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ë²¤ì¹˜ë§ˆí¬ í‰ê°€ì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ í¬ì§€ ì•ŠìŒì„ ë³´ê³ í•©ë‹ˆë‹¤.
* 2023.8.3 ëª¨ë¸ìŠ¤ì½”í”„ì™€ í—ˆê¹… í˜ì´ìŠ¤ì—ì„œ **Qwen-7B**ì™€ **Qwen-7B-Chat**ì„ ëª¨ë‘ ì¶œì‹œí•©ë‹ˆë‹¤. ë˜í•œ í›ˆë ¨ ì„¸ë¶€ ì‚¬í•­ ë° ëª¨ë¸ ì„±ëŠ¥ì„ í¬í•¨í•œ ëª¨ë¸ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ê¸°ìˆ  ë©”ëª¨ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
<br>

## Performance

ìì—°ì–´ ì´í•´, ìˆ˜í•™ì  ë¬¸ì œ í•´ê²°, ì½”ë”© ë“±ì— ëŒ€í•œ ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ì¼ë ¨ì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸(ì˜ˆ: MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH ë“±)ì—ì„œ Qwen-14Bì™€ Qwen-7B(ë” ë§ì€ í† í°ìœ¼ë¡œ í›ˆë ¨ëœ ìƒˆ ë²„ì „ì´ë©° ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ 2048ì—ì„œ 8192ë¡œ í™•ì¥ë¨)ëŠ” ìœ ì‚¬í•œ ëª¨ë¸ í¬ê¸°ì˜ ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Qwen-14Bì¡°ì°¨ë„ GPT-4ëŠ” ë§í•  ê²ƒë„ ì—†ê³  GPT-3.5ì—ë„ í¬ê²Œ ë’¤ë–¨ì–´ì§‘ë‹ˆë‹¤. ì•„ë˜ ê²°ê³¼ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

<p align="left">
    <img src="assets/radar_14b.jpg" width="600"/>
<p>
<br>

| Model              |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |
|:-------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|
|                    |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |
| LLaMA2-7B          |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |
| LLaMA2-13B         |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |
| LLaMA2-34B         |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |
| ChatGLM2-6B        |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |
| InternLM-7B        |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |
| InternLM-20B       |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |
| Baichuan2-7B       |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |
| Baichuan2-13B      |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |
| Qwen-7B (original) |   56.7   |   59.6   |   51.6   |   10.4   |   24.4    |   31.2   |   40.6   |   58.8   |
| **Qwen-7B**        |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |
| **Qwen-14B**       | **66.3** | **72.1** | **61.3** | **24.8** | **32.3**  | **40.8** | **53.4** | **71.0** |


ë¹„êµí•œ ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ê³µì‹ì ìœ¼ë¡œ ë³´ê³ ëœ ê²°ê³¼ì™€ [OpenCompass](https://opencompass.org.cn/leaderboard-llm) ì‚¬ì´ì˜ ìµœê³  ì ìˆ˜ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤. 

ë” ë§ì€ ì‹¤í—˜ ê²°ê³¼(ë” ë§ì€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ìì„¸í•œ ëª¨ë¸ ì„±ëŠ¥) ë° ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf)ë¥¼ í´ë¦­í•˜ì—¬ ê¸°ìˆ  ë³´ê³ ì„œë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

<br><br>

## Requirements

* python 3.8 and above
* pytorch 1.12 and above, 2.0 and above are recommended
* transformers 4.32 and above
* CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)
<br>

## Quickstart

ì•„ë˜ì—ì„œëŠ” ğŸ¤– ëª¨ë¸ìŠ¤ì½”í”„ ë° ğŸ¤— íŠ¸ëœìŠ¤í¬ë¨¸ì™€ í•¨ê»˜ Qwen-Chatì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— í™˜ê²½ì„ ì„¤ì •í•˜ê³  í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ìœ„ì˜ ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í™•ì¸í•œ ë‹¤ìŒ ì¢…ì† ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.

```bash
pip install -r requirements.txt
```

ì¥ì¹˜ì—ì„œ fp16 ë˜ëŠ” bf16ì„ ì§€ì›í•˜ëŠ” ê²½ìš°, íš¨ìœ¨ì„ ë†’ì´ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ [flash-attention](https://github.com/Dao-AILab/flash-attention)(**í˜„ì¬ëŠ” Flash Attention 2ë¥¼ ì§€ì›í•¨**)ì„ ì„¤ì¹˜í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. (**í”Œë˜ì‹œ ì–´í…ì…˜ì€ ì„ íƒ ì‚¬í•­ì´ë©°, ì„¤ì¹˜í•˜ì§€ ì•Šì•„ë„ í”„ë¡œì íŠ¸ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.**).

```bash
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# Below are optional. Installing them might be slow.
# pip install csrc/layer_norm
# pip install csrc/rotary
```

ì´ì œ ëª¨ë¸ìŠ¤ì½”í”„ ë˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ¤— Transformers

ì¶”ë¡ ì— Qwen-Chatì„ ì‚¬ìš©í•˜ë ¤ë©´ ì•„ë˜ì— ì„¤ëª…ëœ ëŒ€ë¡œ ëª‡ ì¤„ì˜ ì½”ë“œë¥¼ ì…ë ¥í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. "Qwen/Qwen-7B-Chat" ë° "Qwen/Qwen-14B-Chat"ê³¼ ê°™ì´ ì˜¬ë°”ë¥¸ ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œë¥¼ ì „ë‹¬í•´ì•¼ í•˜ê³ , **ìµœì‹  ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B-Chat", "Qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# 1st dialogue turn
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚

# 2nd dialogue turn
response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history)
print(response)
# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚
# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚
# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚
# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚
# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚
# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚

# 3rd dialogue turn
response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹
```

ì‚¬ì „ í•™ìŠµëœ ê¸°ë³¸ ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ë„ ê°„ë‹¨í•©ë‹ˆë‹¤.

<details>
  <summary>Running Qwen</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B", "Qwen/Qwen-14B" 
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)
# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

inputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...
```

</details>

HuggingFaceì—ì„œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì™€ ì½”ë“œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë™ì•ˆ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ê²½ìš°, ì•„ë˜ì— ì„¤ëª…ëœ ëŒ€ë¡œ ëª¨ë¸ìŠ¤ì½”í”„ì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¨¼ì € ê°€ì ¸ì˜¨ ë‹¤ìŒ ë¡œì»¬ ë””ë ‰í„°ë¦¬ì—ì„œ ë¡œë“œí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# Downloading model checkpoint to a local dir model_dir
# model_dir = snapshot_download('qwen/Qwen-7B')
# model_dir = snapshot_download('qwen/Qwen-7B-Chat')
# model_dir = snapshot_download('qwen/Qwen-14B')
model_dir = snapshot_download('qwen/Qwen-14B-Chat')

# Loading local checkpoints
# trust_remote_code is still set as True since we still load codes from local dir instead of transformers
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="auto",
    trust_remote_code=True
).eval()
```

### ğŸ¤– ModelScope

ModelScopeëŠ” ì„œë¹„ìŠ¤í˜• ëª¨ë¸(MaaS)ì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”Œë«í¼ìœ¼ë¡œ, AI ê°œë°œìì—ê²Œ ìœ ì—°í•˜ê³  ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ì•„ë˜ì™€ ê°™ì´ ModelScopeë¡œ ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
from modelscope import GenerationConfig

# Model names: "qwen/Qwen-7B-Chat", "qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen-7B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚

response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
response, history = model.chat(tokenizer, "æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ", history=history) 
print(response)
response, history = model.chat(tokenizer, "å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹", history=history)
print(response)
```

### Batch Inference
Qwenì€ ì¼ê´„ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤. í”Œë˜ì‹œ ì–´í…ì…˜ì´ í™œì„±í™”ëœ ìƒíƒœì—ì„œ ì¼ê´„ ì¶”ë¡ ì„ ì‚¬ìš©í•˜ë©´ ì†ë„ê°€ 40% í–¥ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆì œ ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import GenerationConfig
from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids

tokenizer = AutoTokenizer.from_pretrained(
    './',
    pad_token='<|extra_0|>',
    eos_token='<|endoftext|>',
    padding_side='left',
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    './',
    pad_token_id=tokenizer.pad_token_id,
    device_map="auto",
    trust_remote_code=True
).eval()
model.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)

all_raw_text = ["æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚", "ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹", "æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°"]
batch_raw_text = []
for q in all_raw_text:
    raw_text, _ = make_context(
        tokenizer,
        q,
        system="You are a helpful assistant.",
        max_window_size=model.generation_config.max_window_size,
        chat_format=model.generation_config.chat_format,
    )
    batch_raw_text.append(raw_text)

batch_input_ids = tokenizer(batch_raw_text, padding='longest')
batch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)
batch_out_ids = model.generate(
    batch_input_ids,
    return_dict_in_generate=False,
    generation_config=model.generation_config
)
padding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]

batch_response = [
    decode_tokens(
        batch_out_ids[i][padding_lens[i]:],
        tokenizer,
        raw_text_len=len(batch_raw_text[i]),
        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),
        chat_format="chatml",
        verbose=False,
        errors='replace'
    ) for i in range(len(all_raw_text))
]
print(batch_response)

response, _ = model.chat(tokenizer, "æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚", history=None)
print(response)

response, _ = model.chat(tokenizer, "ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹", history=None)
print(response)

response, _ = model.chat(tokenizer, "æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°", history=None)
print(response)
```

### CPU
CPUì— ì €í¬ ëª¨ë¸ì„ ë°°í¬í•˜ë ¤ë©´, Qwenê³¼ tiktokenì„ ìˆœìˆ˜ C++ë¡œ êµ¬í˜„í•œ [qwen.cpp](https://github.com/QwenLM/qwen.cpp)ë¥¼ ì‚¬ìš©í•˜ì‹¤ ê²ƒì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì €ì¥ì†Œì—ì„œ í™•ì¸í•˜ì„¸ìš”!

ë˜í•œ, CPUì—ì„œ ì§ì ‘ ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” ê²ƒë„ ê°„ë‹¨í•˜ì§€ë§Œ ë””ë°”ì´ìŠ¤ ì‚¬ì–‘ì´ í•„ìš”í•©ë‹ˆë‹¤.

```python
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
```

ê·¸ëŸ¬ë‚˜ ì¶”ë¡  íš¨ìœ¨ì„±ì´ ë§¤ìš° ë‚®ì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. (suffer from extremely low inference efficiency)

### Multiple GPUs

GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ì—¬ 1ê°œ ì´ìƒì˜ GPUì—ì„œ ëª¨ë¸ì„ ì‹¤í–‰í•˜ë ¤ëŠ” ê²½ìš°, ì´ì œ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì§€ì›í•˜ëŠ” ê¸°ë³¸ ë¡œë”© ë°©ë²•ì„ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. utils.py`ì— ê¸°ë°˜í•œ ì´ì „ ë°©ë²•ì€ ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ì´ ë°©ë²•ì€ ê°„ë‹¨í•˜ì§€ë§Œ ë„¤ì´í‹°ë¸Œ íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ì²˜ë¦¬ì˜ íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. FastChatê³¼ í•¨ê»˜ vLLMì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë©° ë°°í¬ ì„¹ì…˜ì„ ì½ì–´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.
<br><br>

## Quantization

### GPTQ

[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì†”ë£¨ì…˜ì„ ì œê³µí•˜ê³  ìˆìœ¼ë©°, ë¬´ì†ì‹¤ ëª¨ë¸ íš¨ê³¼ì— ê°€ê¹ì§€ë§Œ ë©”ëª¨ë¦¬ ë¹„ìš©ê³¼ ì¶”ë¡  ì†ë„ ëª¨ë‘ì—ì„œ ì„±ëŠ¥ì´ í–¥ìƒëœ Int4 ì–‘ìí™” ëª¨ë¸ì„ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤.

ì—¬ê¸°ì—ì„œëŠ” ì œê³µëœ ì–‘ìí™” ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì‹œì‘í•˜ê¸° ì „ì— ìë™-gptqì˜ ìš”êµ¬ ì‚¬í•­(ì˜ˆ: í† ì¹˜ 2.0 ì´ìƒ, íŠ¸ëœìŠ¤í¬ë¨¸ 4.32.0 ì´ìƒ ë“±)ì„ ì¶©ì¡±í•˜ê³  í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

```bash
pip install auto-gptq optimum
```

ë§Œì•½ 'auto-gptq' ì„¤ì¹˜ì— ë¬¸ì œê°€ ìˆë‹¤ë©´, ê³µì‹ [repo](https://github.com/PanQiWei/AutoGPTQ)ì—ì„œ íœ ì„ ì°¾ì•„ë³´ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤.

ê·¸ëŸ¬ë©´ ì •ëŸ‰í™”ëœ ëª¨ë¸ì„ ì‰½ê²Œ ë¡œë“œí•˜ê³  í‰ì†Œì™€ ë™ì¼í•˜ê²Œ ì¶”ë¡ ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# Model names: "Qwen/Qwen-7B-Chat-Int4", "Qwen/Qwen-14B-Chat-Int4"
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
response, history = model.chat(tokenizer, "Hi", history=None)
```

ë²¤ì¹˜ë§ˆí¬ì—ì„œ BF16, Int8 ë° Int4 ëª¨ë¸ì˜ ëª¨ë¸ ì„±ëŠ¥ì„ ì‚´í´ë³¸ ê²°ê³¼, ì–‘ìí™”ëœ ëª¨ë¸ì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ í¬ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |
|----------------------|:----:|:-----------:|:-----:|:---------:|
| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |
| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |
| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |
| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |
| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0	 |   48.2    |
| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |

### Quantization of KV cache

> ì°¸ê³ : í—ˆê¹… í˜ì´ìŠ¤ì˜ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì¸í•´ ì´ ê¸°ëŠ¥ì— ëŒ€í•œ ì§€ì› íŒŒì¼
> (ì˜ˆ: `cache_autogptq_cuda_256.cpp` ë° `cache_autogptq_cuda_kernel_245.cu`)ì´ ëˆ„ë½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.
> ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“ˆ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ë„£ìœ¼ì„¸ìš”.

ì£¼ì˜ KV ìºì‹œë¥¼ ì •ëŸ‰í™”í•˜ì—¬ ì••ì¶•í•˜ì—¬ ì €ì¥í•˜ë©´ ìƒ˜í”Œ ì²˜ë¦¬ëŸ‰ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'use_cache_quantization' ë° 'use_cache_kernel' íŒŒë¼ë¯¸í„°ëŠ” KV ìºì‹œ ì–‘ìí™” ë™ì‘ì„ ì œì–´í•˜ê¸° ìœ„í•´ ì œê³µë©ë‹ˆë‹¤.
`use_cache_quantization = True, use_cache_kernel = True`ì¼ ê²½ìš°, kv-cache-quantizationì´ í™œì„±í™”ë©ë‹ˆë‹¤.
êµ¬ì²´ì ì¸ ì‚¬ìš© ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
     device_map="auto",
     trust_remote_code=True,
     use_cache_quantization=True,
     use_cache_kernel=True,
     use_flash_attn=False
)
```
Attention:
í˜„ì¬ kv-ìºì‹œ ì–‘ìí™” ë° í”Œë˜ì‹œ attnì„ ë™ì‹œì— ì¼¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
kv ìºì‹œ ì–‘ìí™”ì™€ use_flash_attnì„ ë™ì‹œì— í™œì„±í™”í•˜ë©´(`use_flash_attn=True, use_cache_quantization=True, use_cache_kernel=True`), use_flash_attnì€ ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤(`use_flash_attn=false`).

ì •ëŸ‰í™”ëœ int8-kvcache ëª¨ë¸ì„ ì‚¬ìš©í•´ë„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í‰ê°€ì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ í¬ì§€ ì•ŠìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í‰ê°€ëŠ” ë©”ëª¨ë¦¬ í’‹í”„ë¦°íŠ¸ì— ì¤‘ì ì„ ë‘ì—ˆìŠµë‹ˆë‹¤. 
í”„ë¡œíŒŒì¼ë§ì€ PyTorch 2.0.1 ë° CUDA 11.4ê°€ íƒ‘ì¬ëœ ë‹¨ì¼ A100-SXM4-80G GPUì—ì„œ ì‹¤í–‰ë˜ì—ˆìœ¼ë©°, BF16 ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©° ê¸°ë³¸ì ìœ¼ë¡œ 1024ê°œì˜ í† í°(seq-length=1024)ì„ ìƒì„±í•˜ì˜€ê³ , oomì€ ë©”ëª¨ë¦¬ ë¶€ì¡±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

kv-ìºì‹œ ì–‘ìí™”ë¥¼ ì¼œë©´ ë” í° ë°°ì¹˜ í¬ê¸°(bs)ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

| USE KVCache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |
|-------------|:------:|:------:|:------:|:------:|:------:|:------:|
| no          | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  oom   |  oom   |
| yes         | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |

kv-ìºì‹œ ì–‘ìí™”ê°€ ì¼œì ¸ ìˆìœ¼ë©´ ëª¨ë¸ì€ ì¶”ë¡  ì‹œ ë” ê¸´ seq ê¸¸ì´(sl, ìƒì„±ëœ í† í° ìˆ˜)ë¥¼ ìƒì„±í•  ë•Œ ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

| USE KVCache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |
|-------------|:------:|:-------:|:-------:|:-------:|:-------:|
| no          | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |
| yes         |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |

kv-ìºì‹œ ì–‘ìí™”ë¥¼ ì¼œëŠ” ëª¨ë¸ì€ ë ˆì´ì–´-íŒ¨ìŠ¤íŠ¸ì˜ í˜•ì‹ì„ floatì—ì„œ int8ë¡œ ë³€í™˜í•˜ê³ , ì–‘ìí™”ëœ ë ˆì´ì–´-íŒ¨ìŠ¤íŠ¸ëŠ” í˜„ì¬ ê°’ì˜ ì–‘ìí™” ë§¤ê°œë³€ìˆ˜ë„ ì €ì¥í•©ë‹ˆë‹¤.
êµ¬ì²´ì ì¸ ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1ã€keyì™€ valueì˜ ì–‘ìí™” (Quantize key/value)
```
    qv,scale,zero_point=quantize_cache_v(v)
```
2ã€layer_pastë¡œ ì–‘ìí™”ëœ key, value ì €ì¥ (Store into layer_past)

quantized layer_pastì˜ í¬ë§·
```
    layer_past=((q_key,key_scale,key_zero_point),
                (q_value,value_scale,value_zero_point))
```
layer_pastì˜ ê¸°ë³¸ í¬ë§·
```
    layer_past=(key,value)
```

ë‹¤ì‹œ floatìœ¼ë¡œ ì–‘ìí™”ëœ attention KVë¥¼ ì‚¬ìš©í•˜ê³ ì í•  ê²½ìš°, int8 key/value ê°’ì„ ë‹¤ìŒê³¼ ê°™ì´ floatìœ¼ë¡œ ì—­ì–‘ìí™”(dequantization)í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
```
    v=dequantize_cache_torch(qv,scale,zero_point)
```
<br>


## Inference Performance

ì´ ì„¹ì…˜ì—ì„œëŠ” ëª¨ë¸ì˜ ì†ë„ ë° ë©”ëª¨ë¦¬ í†µê³„ë¥¼ ë‹¤ì–‘í•œ ì •ë°€ë„ë¡œ ì œê³µí•©ë‹ˆë‹¤. ì†ë„ ë° ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ì€ [ì´ ìŠ¤í¬ë¦½íŠ¸](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Speed

í”Œë˜ì‹œ ì£¼ì˜ v1, v2ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì¡°ê±´ì—ì„œ BF16, Int8, Int4ì˜ ì •ë°€ë„ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 2048ê°œì™€ 8192ê°œì˜ í† í°ì„ ìƒì„±í•  ë•Œì˜ í‰ê·  ì¶”ë¡  ì†ë„ (tokens/s)ë¥¼ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤.

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Precision</th><th rowspan="2">FlashAttn</th><th colspan="2" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">2048</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="9">7B</th><td align="center" rowspan="3">BF16</td><td align="center">v2</td><td align="center">40.93</td><td align="center">36.14</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">40.75</td><td align="center">35.34
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.55</td><td align="center">33.56
    </tr>
    <tr>
        <td align="center" rowspan="3">Int8</td><td align="center">v2</td><td align="center">37.47</td><td align="center">32.54</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">37.51</td><td align="center">32.39
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.84</td><td align="center">32.65
    </tr>
    <tr>
        <td align="center" rowspan="3">Int4</td><td align="center">v2</td><td align="center">50.09</td><td align="center">38.61</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">45.98</td><td align="center">36.47
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">48.12</td><td align="center">36.70
    </tr>
    <tr>
        <th rowspan="9">14B</th><td align="center" rowspan="3">BF16</td><td align="center">v2</td><td align="center">32.88</td><td align="center">24.87</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">32.76</td><td align="center">28.89
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">29.32</td><td align="center">22.91
    </tr>
    <tr>
        <td align="center" rowspan="3">Int8</td><td align="center">v2</td><td align="center">29.28</td><td align="center">24.22</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">28.31</td><td align="center">23.87
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">31.12</td><td align="center">24.60
    </tr>
    <tr>
        <td align="center" rowspan="3">Int4</td><td align="center">v2</td><td align="center">38.72</td><td align="center">27.33</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">37.81</td><td align="center">26.46
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.65</td><td align="center">26.00
    </tr>
</table>


êµ¬ì²´ì ìœ¼ë¡œ í”„ë¡œíŒŒì¼ë§ ì„¤ì •ì€ 2048ê°œì˜ í† í°ì„ ì¸ì½”ë”©í•˜ê³  8192ê°œì˜ ìƒˆë¡œìš´ í† í°ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í”„ë¡œíŒŒì¼ë§ì€ íŒŒì´í† ì¹˜ 2.0.1 ë° CUDA 11.8ì´ ì„¤ì¹˜ëœ ë‹¨ì¼ A100-SXM4-80G GPUì—ì„œ ì‹¤í–‰ë˜ì—ˆìœ¼ë©°, ì¶”ë¡  ì†ë„ëŠ” ì¸ì½”ë”© ë° ìƒì„±ëœ í† í°ì— ëŒ€í•œ í‰ê· ê°’ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.

ì°¸ê³ : ìœ„ì— ì–¸ê¸‰ëœ Int4/Int8 ëª¨ë¸ì˜ ìƒì„± ì†ë„ëŠ” autogptq ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ``AutoModelForCausalLM.from_pretrained``ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œë“œëœ ëª¨ë¸ì˜ ì†ë„ëŠ” í˜„ì¬ ì•½ 20% ëŠë¦½ë‹ˆë‹¤. ì´ ë¬¸ì œëŠ” í—ˆê¹…í˜ì´ìŠ¤ íŒ€ì— ë³´ê³ í–ˆìœ¼ë©°, í•´ê²° ë°©ì•ˆì´ ë§ˆë ¨ë˜ëŠ” ì¦‰ì‹œ ì—…ë°ì´íŠ¸í•˜ê² ìŠµë‹ˆë‹¤.

### GPU Memory Usage

ë˜í•œ 2048ê°œì˜ í† í°ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¸ì½”ë”©í•˜ê³  ë‹¨ì¼ í† í°ì„ ìƒì„±í•  ë•Œì™€ (ë‹¨ì¼ í† í°ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬) 8192ê°œì˜ í† í°ì„ ìƒì„±í•  ë•Œ, ê°ê° BF16, Int8 ë˜ëŠ” Int4 ì–‘ìí™” ìˆ˜ì¤€ì—ì„œ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë„ í”„ë¡œíŒŒì¼ë§í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼(GB)ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Precision</th><th colspan="2" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">2048</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="3">7B</th><td align="center">BF16</td><td align="center">16.99</td><td align="center">22.53</td>
    </tr>
    <tr>
        <td align="center">Int8</td><td align="center">11.20</td><td align="center">16.62
    </tr>
    <tr>
        <td align="center">Int4</td><td align="center">8.21</td><td align="center">13.63</td>
    </tr>
    <tr>
        <th rowspan="3">14B</th><td align="center">BF16</td><td align="center">30.15</td><td align="center">38.94</td>
    </tr>
    <tr>
        <td align="center">Int8</td><td align="center">18.81</td><td align="center">27.54
    </tr>
    <tr>
        <td align="center">Int4</td><td align="center">13.01</td><td align="center">21.79</td>
    </tr>
</table>
<br>


## Finetuning

### Usage
ì´ì œ ì‚¬ìš©ìê°€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ ê³µì‹ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì¸ `finetune.py`ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ ì…¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì œê³µí•˜ì—¬ ê°„í¸í•˜ê²Œ ë¯¸ì„¸ ì¡°ì •ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [DeepSpped](https://github.com/microsoft/DeepSpeed) ë° [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/)ë¥¼ í†µí•œ í•™ìŠµì„ ì§€ì›í•˜ë©°, ì œê³µë˜ëŠ” ì…¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” DeepSpeed(ì°¸ê³ : ìµœì‹  ë²„ì „ì˜ pydanticê³¼ ì¶©ëŒì´ ìˆì„ ìˆ˜ ìˆìŒ)ì™€ PEFTë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì„¤ì¹˜ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```bash
pip install peft deepspeed
```

í•™ìŠµ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ë ¤ë©´ ëª¨ë“  ìƒ˜í”Œì„ ëª©ë¡ì— ë„£ê³  json íŒŒì¼ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤. ê° ìƒ˜í”Œì€ IDì™€ ëŒ€í™” ëª©ë¡ìœ¼ë¡œ êµ¬ì„±ëœ ì‚¬ì „ì´ë©°, ì•„ë˜ëŠ” ìƒ˜í”Œ 1ê°œì— í¬í•¨ëœ ë‚´ìš©ì„ ë³´ì—¬ì£¼ëŠ” ê°„ë‹¨í•œ ì˜ˆì œì…ë‹ˆë‹¤.
```json
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "ä½ å¥½"
      },
      {
        "from": "assistant",
        "value": "æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚"
      }
    ]
  }
]
```

ë°ì´í„° ì¤€ë¹„ í›„ ì œê³µëœ ì…¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ ê²½ë¡œì¸ `$DATA`ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”.

ë¯¸ì„¸ ì¡°ì • ìŠ¤í¬ë¦½íŠ¸ë¥¼ í†µí•´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì „ì²´ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •
- LoRA
- Q-LoRA

ì „ì²´ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •ì„ ìˆ˜í–‰í•˜ë ¤ë©´ ì „ì²´ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤. íŠ¸ë ˆì´ë‹ì„ ì‹œì‘í•˜ë ¤ë©´ ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.

```bash
# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.
sh finetune/finetune_ds.sh
```

ì…¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì˜¬ë°”ë¥¸ ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ, ë°ì´í„° ê²½ë¡œ, ì¶œë ¥ ë””ë ‰í„°ë¦¬ë¥¼ ì§€ì •í•´ì•¼ë§Œ í•˜ë©°, ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” DeepSpeed ZeRO3ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì£¼ì˜í•˜ì„¸ìš”. ë³€ê²½ì„ ì›í•˜ì‹œë©´ `--deepspeed` ì¸ìˆ˜ë¥¼ ì œê±°í•˜ê±°ë‚˜ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ DeepSpeed config json íŒŒì¼ì„ ë³€ê²½í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë˜í•œ, ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í˜¼í•© ì •ë°€ë„ í›ˆë ¨ì„ ì§€ì›í•˜ë¯€ë¡œ, ì‚¬ìš©ì„ ì›í•˜ì‹¤ ê²½ìš° `--bf16 True` ë˜ëŠ” `--fp16 True`ì¸ìë¥¼ ì „ë‹¬í•˜ì„¸ìš”. í˜¼í•© ì •ë°€ë„ í›ˆë ¨(mixed-precision training)ìœ¼ë¡œ ì¸í•´ fp16ì„ ì‚¬ìš©í•  ë•ŒëŠ”, ë°˜ë“œì‹œ DeepSpeedë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì„¸ìš”. 
ê²½í—˜ì ìœ¼ë¡œ ì‚¬ìš© ì¤‘ì¸ ë¨¸ì‹ ì´ bf16ì„ ì§€ì›í•˜ëŠ” ê²½ìš° ì‚¬ì „ í›ˆë ¨ ë° ì •ë ¬ê³¼ ì¼ê´€ëœ í›ˆë ¨ì„ ìœ„í•´ bf16ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë©°, ë”°ë¼ì„œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •í•´ë‘ì—ˆìŠµë‹ˆë‹¤.

ë§ˆì°¬ê°€ì§€ë¡œ LoRAë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤. ì‹œì‘í•˜ê¸° ì „ì— `PEFT`ë¥¼ ì„¤ì¹˜í–ˆëŠ”ì§€ í™•ì¸í•˜ì‹œê³ , ëª¨ë¸Â·ë°ì´í„°Â·ì¶œë ¥ì— ëŒ€í•œ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”. LoRAëŠ” ì–´ëŒ‘í„°ë§Œ ì €ì¥í•˜ê³ , ì–´ëŒ‘í„°ë¥¼ êµ¬ì„±í•˜ëŠ” config json íŒŒì¼ì˜ ê²½ë¡œëŠ” ë¡œë“œí•  ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì°¾ëŠ” ë° ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì§€ì •í•˜ì‹¤ ë•Œ, ìƒëŒ€ ê²½ë¡œë³´ë‹¤ ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤. ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” bf16ê³¼ fp16ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.

```bash
# Single GPU training
sh finetune/finetune_lora_single_gpu.sh
# Distributed training
sh finetune/finetune_lora_ds.sh
```

ì „ì²´ ë§¤ê°œë³€ìˆ˜ ë¯¸ì„¸ ì¡°ì •ê³¼ ë¹„êµí•  ë•Œ LoRA([ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/2106.09685))ëŠ” ì–´ëŒ‘í„° ë ˆì´ì–´ì˜ ë§¤ê°œë³€ìˆ˜ë§Œ ì—…ë°ì´íŠ¸í•˜ê³ , ì›ë˜ì˜ LLMì˜ ë ˆì´ì–´ëŠ” ê³ ì •ëœ ìƒíƒœë¡œ ìœ ì§€ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë©”ëª¨ë¦¬ ë¹„ìš©ì´ í›¨ì”¬ ì ê²Œ ë“¤ê³  ê³„ì‚° ë¹„ìš©ë„ ì ê²Œ ë“­ë‹ˆë‹¤. 

ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸ì— ChatML í˜•ì‹ì—ì„œ ê°€ì ¸ì˜¨ íŠ¹ìˆ˜ í† í°ì— ëŒ€í•œ ì§€ì‹ì´ ì—†ê¸° ë•Œë¬¸ì— LoRAë¥¼ ì‚¬ìš©í•˜ì—¬ ì±„íŒ… ëª¨ë¸(ì˜ˆ: Qwen-7B-Chat) ëŒ€ì‹  ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸(ì˜ˆ: Qwen-7B)ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ëŠ” ê²½ìš° ìŠ¤í¬ë¦½íŠ¸ê°€ ì„ë² ë”© ë° ì¶œë ¥ ë ˆì´ì–´ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ìë™ ì „í™˜í•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”. ë”°ë¼ì„œ ëª¨ë¸ì´ í† í°ì„ ì´í•´í•˜ê³  ì˜ˆì¸¡í•˜ë ¤ë©´ ì„ë² ë”© ë° ì¶œë ¥ ë ˆì´ì–´ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ë§Œ í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§í•´, í•™ìŠµì„ í†µí•´ LoRAì—ì„œ íŠ¹ìˆ˜ í† í°ì„ ê°€ì ¸ì˜¤ëŠ” ê²½ìš° ì½”ë“œ ë‚´ì—ì„œ `modules_to_save`ë¥¼ ì„¤ì •í•˜ì—¬ ë ˆì´ì–´ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í˜•íƒœë¡œ ë‹¤ì‹œ ì„¤ì •í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ, ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ë¥¼ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” ê²½ìš° ZeRO3ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ZeRO2ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ê²½ìš° DeepSpeed êµ¬ì„± íŒŒì¼ì„ ë³€ê²½í•˜ì—¬ ZeRO3ë¡œ ì „í™˜í•˜ì‹¤ ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ¬í•œ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° LoRAì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ìƒë‹¹í•œ ì°¨ì´ê°€ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë©”ëª¨ë¦¬ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° LoRAì—ì„œ ì±„íŒ… ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”. 

ë§Œì•½ LoRA ë“±ì„ ì‚¬ìš©í•´ë„ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë‹¤ë©´ ì–‘ìí™”ëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ê³¼ í˜ì´ì§• ì£¼ì˜ì™€ ê°™ì€ ê¸°íƒ€ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ë¹„ìš©ì„ í›¨ì”¬ ë” ì ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Q-LoRA([ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/2305.14314))ë¥¼ ê³ ë ¤í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ì°¸ê³ : ë‹¨ì¼ GPU Q-LoRA íŠ¸ë ˆì´ë‹ì„ ì‹¤í–‰í•˜ë ¤ë©´ `pip` ë˜ëŠ” `conda`ë¥¼ í†µí•´ `mpi4py`ë¥¼ ì„¤ì¹˜í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Q-LoRAë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì„¸ìš”.

```bash
# Single GPU training
sh finetune/finetune_qlora_single_gpu.sh
# Distributed training
sh finetune/finetune_qlora_ds.sh
```

Q-LoRAì˜ ê²½ìš°, ì œê³µëœ ì •ëŸ‰í™”ëœ ëª¨ë¸(ì˜ˆ: Qwen-7B-Chat-Int4)ì„ ë¡œë“œí•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì „ì²´ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì • ë° LoRAì™€ ë‹¬ë¦¬ **Q-LoRAì—ëŠ” fp16ë§Œ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— bf16 ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.** ì „ì²´ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì • ë° LoRAì™€ ë‹¬ë¦¬ Q-LoRAì—ëŠ” fp16ë§Œ ì§€ì›ë©ë‹ˆë‹¤. ë‹¨ì¼ GPU í›ˆë ¨ì˜ ê²½ìš° torch ampë¡œ ì¸í•œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— í˜¼í•© ì •ë°€ë„ í›ˆë ¨ì—ëŠ” deepspeedë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ Q-LoRAì˜ ê²½ìš°ì—ë„ ìœ„ì—ì„œ ì–¸ê¸‰í•œ LoRAì˜ íŠ¹ìˆ˜ í† í°ì— ëŒ€í•œ ë¬¸ì œê°€ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì €í¬ëŠ” ì±„íŒ… ëª¨ë¸ì— Int4 ëª¨ë¸ë§Œ ì œê³µí•˜ê³ , ì¼ë°˜ ëª¨ë¸ì˜ ê²½ìš° ChatML í˜•ì‹ì˜ íŠ¹ìˆ˜ í† í°ì„ ì´ë¯¸ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì— ë ˆì´ì–´ì— ëŒ€í•œ ê±±ì •ì€ í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤. ë‹¨, Int4 ëª¨ë¸ì˜ ë ˆì´ì–´ëŠ” í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ í•™ìŠµì— íŠ¹ìˆ˜ í† í°ì„ ë„ì…í•˜ë©´ Q-LoRAê°€ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> ì°¸ê³ : í—ˆê¹… í˜ì´ìŠ¤ì˜ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì¸í•´ íŠ¹ì • íŒŒì´ì¬ì´ ì•„ë‹Œ íŒŒì¼(ì˜ˆ: `*.cpp` ë° `*.cu`) 
> ë“¤ì´ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì¼ì´ í¬í•¨ëœ ë””ë ‰í„°ë¦¬ì— ìˆ˜ë™ìœ¼ë¡œ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•´ì•¼ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì „ì²´ ë§¤ê°œë³€ìˆ˜ ë¯¸ì„¸ ì¡°ì •ê³¼ ë‹¬ë¦¬ LoRA ë° Q-LoRAë¥¼ í†µí•œ í•™ìŠµì€ ì–´ëŒ‘í„°ì˜ ë§¤ê°œë³€ìˆ˜ë§Œì„ ì €ì¥í•©ë‹ˆë‹¤. Qwen-7Bì—ì„œ í•™ìŠµì´ ì‹œì‘ëœë‹¤ê³  ê°€ì •í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì¶”ë¡ ì„ ìœ„í•œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()
```

ì–´ëŒ‘í„°ë¥¼ ë³‘í•©í•˜ê³  ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ë…ë¦½í˜• ëª¨ë¸ë¡œ ì €ì¥í•˜ë ¤ë©´ ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤. (ì´ ì‘ì—…ì€ LoRAì—ì„œë§Œ ê°€ëŠ¥í•˜ë©° Q-LoRAì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ë³‘í•©í•  ìˆ˜ ì—†ìŒ)
```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()

merged_model = model.merge_and_unload()
# max_shard_size and safe serialization are not necessary. 
# They respectively work for sharding checkpoint and save the model to safetensors
merged_model.save_pretrained(new_model_directory, max_shard_size="2048MB", safe_serialization=True)
```

`new_model_directory` í´ë”ì—ëŠ” ë³‘í•©ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ëª¨ë“ˆ íŒŒì¼ì´ í¬í•¨ë˜ê³ , ì €ì¥ëœ íŒŒì¼ì— `*.cu` ë° `*.cpp` íŒŒì¼ì´ ëˆ„ë½ë  ìˆ˜ ìˆìŒì„ ìœ ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. KV ìºì‹œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ í•´ë‹¹ íŒŒì¼ì„ ìˆ˜ë™ìœ¼ë¡œ ë³µì‚¬í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ë˜í•œ, ì´ ë‹¨ê³„ì—ì„œ í† í°í™”ê¸° íŒŒì¼ì€ ìƒˆ ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, tokenizer filesì„ ë³µì‚¬í•˜ê±°ë‚˜ ë‹¤ìŒ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    path_to_adapter, # path to the output directory
    trust_remote_code=True
)

tokenizer.save_pretrained(new_model_directory)
```


ì°¸ê³ : ë©€í‹° GPU íŠ¸ë ˆì´ë‹ì˜ ê²½ìš°, ë¨¸ì‹ ì— ë”°ë¼ ë¶„ì‚° íŠ¸ë ˆì´ë‹ì— ì í•©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì •í•´ì•¼ í•˜ê³ , ë°ì´í„°Â·ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰Â·í›ˆë ¨ ì†ë„ ë“±ì„ ê³ ë ¤í•˜ì—¬  `--model_max_length` ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

### Profiling of Memory and Speed
ë‹¨ì¼ GPU íŠ¸ë ˆì´ë‹ ì„¤ì •ì—ì„œ LoRAì™€ Q-LoRAì˜ GPU ë©”ëª¨ë¦¬ì™€ íŠ¸ë ˆì´ë‹ ì†ë„ë¥¼ í”„ë¡œíŒŒì¼ë§í•©ë‹ˆë‹¤. (LoRA(emb)ëŠ” ì„ë² ë”© ë° ì¶œë ¥ ë ˆì´ì–´ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, LoRAì—ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”© ë° ì¶œë ¥ ë ˆì´ì–´ê°€ ì—†ìŒì„ ìœ ì˜) ì´ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ë‹¨ì¼ A100-SXM4-80G GPUì—ì„œ ì‹¤í—˜í•˜ê³ , CUDA 11.8 ë° Pytorch 2.0ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Flash Attention-2ê°€ ì ìš©ë˜ê³ , ë°°ì¹˜ í¬ê¸° 1ê³¼ ê·¸ë¼ë°ì´ì…˜ ëˆ„ì  8ì„ ê· ì¼í•˜ê²Œ ì‚¬ìš©í•˜ì—¬ 256, 512, 1024, 2048, 4096, 8192 ë“± ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì…ë ¥ì— ëŒ€í•œ ë©”ëª¨ë¦¬(GB)ì™€ ì†ë„(s/iter)ë¥¼ í”„ë¡œíŒŒì¼ë§í•©ë‹ˆë‹¤. ë˜í•œ, 2ê°œì˜ A100 GPUì—ì„œ Qwen-7Bë¥¼ ì‚¬ìš©í•œ ì „ì²´ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •ì— ëŒ€í•œ í†µê³„ë„ ì œê³µí•©ë‹ˆë‹¤. GPU ë©”ëª¨ë¦¬ì˜ ì œí•œìœ¼ë¡œ ì¸í•´ 256, 512, 1024 í† í°ì˜ í†µê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Method</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">256</th><th align="center">512</th><th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="4">7B</th><td>LoRA</td><td align="center">20.1G / 1.2s/it</td><td align="center">20.4G / 1.5s/it</td><td align="center">21.5G / 2.8s/it</td><td align="center">23.8G / 5.2s/it</td><td align="center">29.7G / 10.1s/it</td><td align="center">36.6G / 21.3s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">33.7G / 1.4s/it</td><td align="center">34.1G / 1.6s/it</td><td align="center">35.2G / 2.9s/it</td><td align="center">35.1G / 5.3s/it</td><td align="center">39.2G / 10.3s/it</td><td align="center">48.5G / 21.7s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">11.5G / 3.0s/it</td><td align="center">11.5G / 3.0s/it</td><td align="center">12.3G / 3.5s/it</td><td align="center">13.9G / 7.0s/it</td><td align="center">16.9G / 11.6s/it</td><td align="center">23.5G / 22.3s/it</td>
    </tr>
    <tr>
        <td>Full-parameter</td><td align="center">139.2G / 4.0s/it</td><td align="center">148.0G / 4.0s/it</td><td align="center">162.0G / 4.5s/it</td><td align="center">-</td><td align="center">-</td><td align="center">-</td>
    </tr>
    <tr>
        <th rowspan="3">14B</th><td>LoRA</td><td align="center">34.6G / 1.6s/it</td><td align="center">35.1G / 2.4s/it</td><td align="center">35.3G / 4.4s/it</td><td align="center">37.4G / 8.4s/it</td><td align="center">42.5G / 17.0s/it</td><td align="center">55.2G / 36.0s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">51.2 / 1.7s/it</td><td align="center">51.1G / 2.6s/it</td><td align="center">51.5G / 4.6s/it</td><td align="center">54.1G / 8.6s/it</td><td align="center">56.8G / 17.2s/it</td><td align="center">67.7G / 36.3s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">18.7G / 5.3s/it</td><td align="center">18.4G / 6.3s/it</td><td align="center">18.9G / 8.2s/it</td><td align="center">19.9G / 11.8s/it</td><td align="center">23.0G / 20.1s/it</td><td align="center">27.9G / 38.3s/it</td>
    </tr>
</table>
<br>

## Deployment

### vLLM 
ë°°í¬ ë° ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•´ì„œëŠ” FastChatê³¼ í•¨ê»˜ vLLMì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë¨¼ì € íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
```bash
pip install vllm
pip install "fschat[model_worker,webui]"
```
ë˜ëŠ” `git clone` ë° `pip install -e .`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„¤ì¹˜ ì‹œ ë¬¸ì œê°€ ë°œìƒí•˜ë©´ í•´ë‹¹ ë¬¸ì„œë¥¼ ì½ì–´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤. 

vLLM ë° FastChatê³¼ í•¨ê»˜ Qwenì„ ì‹¤í–‰í•˜ë ¤ë©´ ë¨¼ì € ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
```bash
python -m fastchat.serve.controller
```

ê·¸ëŸ° ë‹¤ìŒ ëª¨ë¸ ì›Œì»¤ë¥¼ ì‹œì‘í•˜ë©´ ì¶”ë¡ ì„ ìœ„í•´ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨ì¼ GPU ì¶”ë¡ ì˜ ê²½ìš° ì§ì ‘ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code
```
ê·¸ëŸ¬ë‚˜ ë” ë¹ ë¥¸ ì¶”ë¡ ì´ë‚˜ ë” í° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•´ ì—¬ëŸ¬ GPUì—ì„œ ëª¨ë¸ì„ ì‹¤í–‰í•˜ë ¤ëŠ” ê²½ìš° vLLMì—ì„œ ì§€ì›í•˜ëŠ” ë³‘ë ¬ ì²˜ë¦¬ í…ì„œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 4ê°œì˜ GPUì—ì„œ ëª¨ë¸ì„ ì‹¤í–‰í•œë‹¤ê³  ê°€ì •í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4
```

ëª¨ë¸ ì›Œì»¤ë¥¼ ì‹¤í–‰í•œ í›„ ì›í•˜ëŠ” ëŒ€ë¡œ ì›¹ ë°ëª¨ ë˜ëŠ” OpenAI APIë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›¹ ë°ëª¨ì˜ ê²½ìš° ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
```bash
python -m fastchat.serve.gradio_web_server
```
OpenAI APIì˜ ê²½ìš°, ë¨¼ì € ì„¤ì¹˜ì— ëŒ€í•œ OpenAI API ì„¤ëª…ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”. ê·¸ëŸ° ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
```bash
python -m fastchat.serve.openai_api_server --host localhost --port 8000
```
<br>

## Demo

### Web UI

íŠ¸ìœ„í„°ì—ì„œëŠ” ì‚¬ìš©ìê°€ ì›¹ UI ë°ëª¨ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” ì½”ë“œë¥¼ ì œê³µ(@wysaid)í•©ë‹ˆë‹¤. ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

```
pip install -r requirements_web_demo.txt
```

ê·¸ëŸ° ë‹¤ìŒ ì•„ë˜ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê³  ìƒì„±ëœ ë§í¬ë¥¼ í´ë¦­í•©ë‹ˆë‹¤.

```bash
python web_demo.py
```

<p align="center">
    <br>
    <img src="assets/web_demo.gif" width="600" />
    <br>
<p>

### CLI Demo

ìƒì„±ì— ëŒ€í•œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ì§€ì›í•˜ëŠ” CLI ë°ëª¨ ì˜ˆì œë¥¼ `cli_demo.py`ì—ì„œ ì œê³µí•©ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ Qwen-7B-Chatê³¼ ìƒí˜¸ ì‘ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ëª¨ë¸ì€ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œ ëª¨ë¸ ì¶œë ¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

```bash
python cli_demo.py
```

<p align="center">
    <br>
    <img src="assets/cli_demo.gif" width="600" />
    <br>
<p>
<br>

## API

APIë¥¼ í†µí•´ Qwenì„ ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ì•Œë¦¬ë°”ë°” í´ë¼ìš°ë“œë¥¼ í†µí•œ DashScope API ì„œë¹„ìŠ¤ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ìì²´ ì„œë²„ì— OpenAI ìŠ¤íƒ€ì¼ APIë¥¼ ë°°í¬í•  ìˆ˜ ìˆëŠ” ìŠ¤í¬ë¦½íŠ¸ë„ ì œê³µí•©ë‹ˆë‹¤.

### DashScope
DashScopeëŠ” ì•Œë¦¬ë°”ë°” í´ë¼ìš°ë“œì—ì„œ ì œê³µí•˜ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ API ì„œë¹„ìŠ¤ë¡œ, ì´ì œ Qwenì„ ì§€ì›í•©ë‹ˆë‹¤. DashScopeì˜ ëª¨ë¸ì€ í˜„ì¬ ì„¸ë¶€ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•ŠëŠ” ì‚¬ë‚´ ë²„ì „ì…ë‹ˆë‹¤. ì´ ì„œë¹„ìŠ¤ì—ëŠ” 'qwen-turbo'ì™€ 'qwen-plus'ê°€ ìˆìœ¼ë©°, ì „ìëŠ” ë” ë¹ ë¥´ê²Œ ì‹¤í–‰ë˜ê³  í›„ìëŠ” ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ê³µì‹ ë¬¸ì„œ](https://dashscope.aliyun.com)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

[ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn)ì—ì„œ ëŒ€ì‹œìŠ¤ì½”í”„ ê³„ì •ì„ ìƒì„±í•˜ê³  API í‚¤(AK)ë¥¼ ë°œê¸‰ë°›ìœ¼ì‹œê³ , í™˜ê²½ ë³€ìˆ˜ë¡œ AKë¥¼ ì„¤ì •í•˜ì‹œëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
```bash
export DASHSCOPE_API_KEY="YOUR_DASHSCOPE_API_KEY"
```
ê·¸ëŸ° ë‹¤ìŒ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  [ë‹¤ìŒ ë¬¸ì„œ](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk)ë¥¼ í™•ì¸í•˜ì„¸ìš”. Pythonì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, pipë¡œ DashScopeë¥¼ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```bash
pip install dashscope
```
JAVA SDKë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì´ ë°©ë²•ìœ¼ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```xml
<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>dashscope-sdk-java</artifactId>
    <version>the-latest-version</version>
</dependency>
```
DashScopeë¥¼ ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ OpenAI APIì™€ ìœ ì‚¬í•œ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì•„ë˜ì— ì˜ˆì‹œê°€ ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤.
```python
import random
from http import HTTPStatus
from dashscope import Generation


def call_with_messages():
    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},
                {'role': 'user', 'content': 'å¦‚ä½•åšè¥¿çº¢æŸ¿é¸¡è›‹ï¼Ÿ'}]
    gen = Generation()
    response = gen.call(
        Generation.Models.qwen_turbo,
        messages=messages,
        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set
        result_format='message',  # set the result to be "message" format.
    )
    return response


if __name__ == '__main__':
    response = call_with_messages()
    if response.status_code == HTTPStatus.OK:
        print(response)
    else:
        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (
            response.request_id, response.status_code,
            response.code, response.message
        ))
```
ìì„¸í•œ ì‚¬ìš©ë²•ì€ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”.

### OpenAI API

OpenAI APIë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¡œì»¬ APIë¥¼ ë°°í¬í•˜ëŠ” ë°©ë²•ì„ ì œê³µ(@hanpenggit)í•©ë‹ˆë‹¤. ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.

```bash
pip install fastapi uvicorn openai "pydantic>=2.3.0" sse_starlette
```

ê·¸ëŸ° ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì—¬ APIë¥¼ ë°°í¬í•©ë‹ˆë‹¤:
```bash
python openai_api.py
```

ì¸ìˆ˜ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì˜ˆ: ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œì˜ ê²½ìš° `-c`, CPU ë°°í¬ì˜ ê²½ìš° `--cpu-only` ë“±). API ë°°í¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ë° ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°, íŒ¨í‚¤ì§€ë¥¼ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë©´ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

API ì‚¬ìš© ë°©ë²•ë„ ê°„ë‹¨í•©ë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

```python
import openai
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "none"

# create a request activating streaming response
for chunk in openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=True 
    # Specifying stop words in streaming output format is not yet supported and is under development.
):
    if hasattr(chunk.choices[0].delta, "content"):
        print(chunk.choices[0].delta.content, end="", flush=True)

# create a request not activating streaming response
response = openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=False,
    stop=[] # You can add custom stop words here, e.g., stop=["Observation:"] for ReAct prompting.
)
print(response.choices[0].message.content)
```

<p align="center">
    <br>
    <img src="assets/openai_api.gif" width="600" />
    <br>
<p>

**í•¨ìˆ˜ í˜¸ì¶œ**ë„ ì§€ì›ë©ë‹ˆë‹¤. í˜„ì¬ëŠ” `stream=False`ì¼ ë•Œë§Œ ì§€ì›í•˜ë©°, [ì‚¬ìš© ì˜ˆì‹œ](examples/function_call_examples.py)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
<br><br>



## Tool Usage

Qwen-Chatì€ ë„êµ¬ ì‚¬ìš©ê³¼ í•¨ìˆ˜ í˜¸ì¶œ ê¸°ëŠ¥ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ì—ì´ì „íŠ¸, LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìœ¼ë©°, íŒŒì´ì¬ ì½”ë“œ ì¸í„°í”„ë¦¬í„°ë¡œ Qwenì„ ë³´ê°•í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

ë¦¬ì•¡íŠ¸ í”„ë¡¬í”„íŠ¸ ì›ì¹™ì— ë”°ë¼ ë„êµ¬ í˜¸ì¶œì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì„¤ëª…ì„œëŠ” [ë¦¬ì•¡íŠ¸ ì˜ˆì œ](examples/react_prompt.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. ì´ ì›ì¹™ì— ë”°ë¼ [openai_api.py](openai_api.py)ì—ì„œ í•¨ìˆ˜ í˜¸ì¶œì„ ì§€ì›í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì¤‘êµ­ ì˜¤í”ˆì†ŒìŠ¤ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ëª¨ë¸ì˜ ë„êµ¬ í˜¸ì¶œ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼, Qwen-Chatì€ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

<table>
    <tr>
        <th colspan="4" align="center">Chinese Tool-Use Benchmark</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selection (Acc.â†‘)</th><th align="center">Tool Input (Rouge-Lâ†‘)</th><th align="center">False Positive Errorâ†“</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">95%</td><td align="center">0.90</td><td align="center">15.0%</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">85%</td><td align="center">0.88</td><td align="center">75.0%</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">98%</td><td align="center">0.91</td><td align="center">7.3%</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">98%</td><td align="center">0.93</td><td align="center">2.4%</td>
    </tr>
</table>

ìˆ˜í•™ ë¬¸ì œ í•´ê²°, ë°ì´í„° ì‹œê°í™”, íŒŒì¼ ì²˜ë¦¬ ë° ì›¹ ìŠ¤í¬ë˜í•‘ê³¼ ê°™ì€ ê¸°íƒ€ ë²”ìš© ì‘ì—…ì— Python ì½”ë“œ ì¸í„°í”„ë¦¬í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” Qwenì˜ ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì´ëŸ¬í•œ ê¸°ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ íŠ¹ë³„íˆ ê³ ì•ˆëœ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë§Œë“¤ì–´ ì˜¤í”ˆ ì†ŒìŠ¤í™”í–ˆìŠµë‹ˆë‹¤. ë²¤ì¹˜ë§ˆí¬ëŠ” ì´ [ë§í¬](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Qwenì€ ì½”ë“œ ìƒì„± ì‹œ ì½”ë“œ ì‹¤í–‰ì„±ê³¼ ê²°ê³¼ ì •í™•ë„ ì¸¡ë©´ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

<table>
    <tr>
        <th colspan="4" align="center">Executable Rate of Generated Code (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Mathâ†‘</th><th align="center">Visualizationâ†‘</th><th align="center">Generalâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">91.9</td><td align="center">85.9</td><td align="center">82.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">89.2</td><td align="center">65.0</td><td align="center">74.1</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">33.1</td>
        <td align="center">24.1 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">50.0</td>
        <td align="center">40.5</td>
        <td align="center">48.3 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">85.1</td>
        <td align="center">54.0</td>
        <td align="center">70.7 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">93.2</td>
        <td align="center">55.8</td>
        <td align="center">74.1 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">78.4</td>
        <td align="center">44.2</td>
        <td align="center">62.1 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">70.3</td>
        <td align="center">44.2</td>
        <td align="center">65.5 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">82.4</td>
        <td align="center">64.4</td>
        <td align="center">67.2 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">89.2</td>
        <td align="center">84.1</td>
        <td align="center">65.5</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">Accuracy of Code Execution Results (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Mathâ†‘</th><th align="center">Visualization-Hardâ†‘</th><th align="center">Visualization-Easyâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">82.8</td><td align="center">66.7</td><td align="center">60.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">47.3</td><td align="center">33.3</td><td align="center">55.7</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">3.9</td>
        <td align="center">14.3</td>
        <td align="center">39.2 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">8.3</td>
        <td align="center">8.3</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">14.3</td>
        <td align="center">26.2</td>
        <td align="center">60.8 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">28.2</td>
        <td align="center">27.4</td>
        <td align="center">62.0 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">28.5</td>
        <td align="center">4.8</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">34.6</td>
        <td align="center">21.4</td>
        <td align="center">45.6 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">40.5</td>
        <td align="center">54.4 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">58.4</td>
        <td align="center">53.6</td>
        <td align="center">59.5</td>
    </tr>
</table>

<p align="center">
    <br>
    <img src="assets/code_interpreter_showcase_001.jpg" />
    <br>
<p>

ë˜í•œ, ì €í¬ ëª¨ë¸ì´ í—ˆê¹…í˜ì´ìŠ¤ ì—ì´ì „íŠ¸ ì—­í• ì„ í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•˜ëŠ” ì‹¤í—˜ ê²°ê³¼ë„ ì œê³µí•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì˜ˆì œ ë¬¸ì„œ](examples/transformers_agent.md)ë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì œê³µí•˜ëŠ” í‰ê°€ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agent Benchmark- Run Mode</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selectionâ†‘</th><th align="center">Tool Usedâ†‘</th><th align="center">Codeâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">100</td><td align="center">100</td><td align="center">97.4</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">95.4</td><td align="center">96.3</td><td align="center">87.0</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">86.1</td><td align="center">87.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">87.0</td><td align="center">88.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">87.0</td><td align="center">87.0</td><td align="center">71.5</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">93.5</td><td align="center">94.4</td><td align="center">87.0</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agent Benchmark - Chat Mode</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selectionâ†‘</th><th align="center">Tool Usedâ†‘</th><th align="center">Codeâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">98.5</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">97.3</td><td align="center">96.8</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">91.1</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">94.7</td><td align="center">94.7</td><td align="center">85.1</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">95.5</td>
    </tr>
</table>

<br>

## Long-Context Understanding
ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ í™•ì¥í•˜ê³  í›ˆë ¨ ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ ë³‘ëª© í˜„ìƒì„ í•´ì†Œí•˜ê¸° ìœ„í•´ NTK ì¸ì‹ ë³´ê°„, ìœˆë„ìš° ì£¼ì˜, LogN ì£¼ì˜ ìŠ¤ì¼€ì¼ë§ ë“± ì—¬ëŸ¬ ê¸°ë²•ì„ ë„ì…í•˜ì—¬ Qwen-7B/14Bì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ 2kì—ì„œ 8k í† í° ì´ìƒìœ¼ë¡œ, Qwen-7BëŠ” 8kì—ì„œ 32k í† í°ìœ¼ë¡œ í™•ì¥í•©ë‹ˆë‹¤. PPL í‰ê°€ì™€ í•¨ê»˜ arXiv ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì–¸ì–´ ëª¨ë¸ë§ ì‹¤í—˜ì„ ìˆ˜í–‰í•œ ê²°ê³¼, ê¸´ ì»¨í…ìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ Qwenì´ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<table>
    <tr>
        <th rowspan="2">Model</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th><th align="center">32768</th>
    </tr>
     <tr>
        <td>Qwen-7B (original)</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.56</td><td align="center">4.62</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.49</td><td align="center">4.32</td><td align="center">-</td>
    </tr>
    <tr>
    <tr>
        <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center">7.27</td><td align="center">181.49</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.33</b></td><td align="center"><b>3.22</b></td><td align="center"><b>3.17</b></td>
    </tr>
    <tr>
        <td>Qwen-14B</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center">22.79</td><td align="center">334.65</td><td align="center">3168.35</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center"><b>3.29</b></td><td align="center"><b>3.18</b></td><td align="center">3.42</td><td align="center">-</td>
    </tr>
</table>


## Tokenizer

tiktoken ê¸°ë°˜ì˜ tokenizerëŠ” ë¬¸ì¥ ë‹¨ìœ„ tokenizerì™€ ê°™ì€ ë‹¤ë¥¸ tokenizerì™€ ë‹¤ë¦…ë‹ˆë‹¤. íŠ¹íˆ ë¯¸ì„¸ ì¡°ì • ì‹œ íŠ¹ìˆ˜ í† í°(special tokens)ì„ ì œëŒ€ë¡œ ì²˜ë¦¬í•´ì•¼í•©ë‹ˆë‹¤. tokenizerì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ê³¼ ë¯¸ì„¸ ì¡°ì • ê´€ë ¨ ì‚¬ìš©ë²•ì€ [tokenizer ê´€ë ¨ ë¬¸ì„œ](tokenization_note.md)ë¥¼ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

<br><br>

## Reproduction

ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ ì¬í˜„í•  ìˆ˜ ìˆë„ë¡ ê²°ê³¼ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [eval/EVALUATION.md](eval/EVALUATION.md)ë¥¼ í™•ì¸í•˜ì„¸ìš”. ì¬í˜„ ê²°ê³¼ëŠ” ë³´ê³ ëœ ê²°ê³¼ì™€ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
<br><br>

## FAQ

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ìƒˆ ì´ìŠˆë¥¼ ì‹œì‘í•˜ê¸° ì „ì— ë¨¼ì € [ìì£¼ ë¬»ëŠ” ì§ˆë¬¸](FAQ.md)ê³¼ ì´ìŠˆë¥¼ ì°¸ì¡°í•˜ì—¬ í•´ê²° ë°©ë²•ì„ ì°¾ì•„ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.
<br><br>

## Citation
ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´, ë‹¤ìŒì„ ììœ ë¡­ê²Œ ì¸ìš©í•´ì£¼ì„¸ìš”.

```
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
```
<br>

## License Agreement

ì—°êµ¬ìì™€ ê°œë°œìëŠ” Qwenê³¼ Qwen-Chatì˜ ì½”ë“œì™€ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ìƒì—…ì  ì‚¬ìš©ë„ í—ˆìš©ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE)ì—ì„œ í™•ì¸í•˜ì„¸ìš”. ìƒì—…ì  ì‚¬ìš©ì— ëŒ€í•œ ìš”êµ¬ ì‚¬í•­ì´ ìˆëŠ” ê²½ìš° ì–‘ì‹ ([7B](https://dashscope.console.aliyun.com/openModelApply/qianwen), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat))ì„ ì‘ì„±í•˜ì—¬ ì‹ ì²­í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
<br><br>

## Contact Us

ì €í¬ ë¦¬ì„œì¹˜íŒ€ì´ë‚˜ ì œí’ˆíŒ€ì— ë©”ì‹œì§€ë¥¼ ë‚¨ê¸°ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´, Discord ë˜ëŠ” WeChat ê·¸ë£¹ì— ê°€ì…í•˜ì‹œê±°ë‚˜ qianwen_opensource@alibabacloud.comë¡œ ì´ë©”ì¼ì„ ë‚¨ê²¨ì£¼ì„¸ìš”.